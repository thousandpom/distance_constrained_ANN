{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This script contains the source code for generating figures used in ```NSCMasterThesis_XinyueYao.pdf``` to demonstrate results of a recurrent neural network. Users need to install necessary libraries and initialize the inference before creating figures.\n",
    "\n",
    "In all functions:\n",
    "* $\\beta=0.0$ indicates an $L^p$ regularized model\n",
    "* $\\beta=1.0$ indicates a distance-constrained model\n",
    "\n",
    "Relevant figures are:\n",
    "\n",
    "* [Fig. 3.3](#acc)\n",
    "* [Fig. 3.4 & A.2](#trial)\n",
    "* [Fig. 3.5](#ed)\n",
    "* [Fig. 3.6](#conn)\n",
    "* [Fig. 3.7](#connd)\n",
    "* [Fig. A.3](#wd)\n",
    "* [Fig. A.4](#cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries required to run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import os\n",
    "%matplotlib notebook\n",
    "from matplotlib.ticker import (MultipleLocator)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "import collections\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import utils\n",
    "from model import ConstrainedModel\n",
    "from generate_input import get_data\n",
    "from scipy import stats\n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator)\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the json file and initialize the model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"./hps.json\"\n",
    "hps = utils.Params(json_path)\n",
    "model = ConstrainedModel(hps.n_bits, hps.hidden_size, hps.n_bits, hps.n_spatial_dims, hps.norm)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alpha_path(path):\n",
    "    # Obtains the path for all checkpoints in a sorted list\n",
    "    alpha_path = list(glob(os.path.join(path, 'alpha_*')))\n",
    "    file_paths = []\n",
    "    for idx in range(len(alpha_path)):\n",
    "        file_path = list(glob(os.path.join(str(alpha_path[idx]))))#, 'checkpoints'\n",
    "        if len(file_path)!=1:\n",
    "            raise ValueError(f'File path should receive one element: {file_path}')\n",
    "        file_path = file_path[0]\n",
    "        file_paths.append(file_path)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd(model, ckpt_file):\n",
    "    ckpt = utils.load_checkpoint(ckpt_file, model)\n",
    "    weight = []\n",
    "    dist = []\n",
    "    for w, d in zip(model.layers_weight, model.distances):\n",
    "        weight += [torch.flatten(w.data).numpy()]\n",
    "        dist += [torch.flatten(d.data).numpy()]\n",
    "    weight = np.concatenate(weight)\n",
    "    dist = np.concatenate(dist)\n",
    "    return model, weight, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_trial(trial, fn, alpha, beta, **kwargs):\n",
    "    json_path = \"./hps.json\"\n",
    "    hps = utils.Params(json_path)\n",
    "    model = ConstrainedModel(hps.n_bits, hps.hidden_size, hps.n_bits, hps.n_spatial_dims, hps.norm)\n",
    "    \n",
    "    list_metric = []\n",
    "    files_seed = list(glob(f'./trial{trial}/seed*trained_model/alpha_{alpha}_beta_{beta}/checkpoints/last.pth'))\n",
    "    for filename in files_seed:\n",
    "        _, h_w, _ = get_wd(model, filename)\n",
    "        list_metric.append(fn(h_w, **kwargs))\n",
    "    return np.median(list_metric), np.std(list_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_test(a, b, thres=0.05):\n",
    "    d, p = stats.ks_2samp(a, b)\n",
    "    return d, p, p < thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remove nodes weights associated with nodes that only have self-connection but are not connected to other nodes. '''\n",
    "def trim_wt(ckpt_path, hps, threshold=1e-8):\n",
    "    alpha = ckpt_path.split('alpha_')[-1].split('_beta')[0]   \n",
    "#     for idx in range(len(alpha_paths)):\n",
    "    model = ConstrainedModel(hps.n_bits, 64, hps.n_bits, hps.n_spatial_dims, hps.norm)\n",
    "    # Reload checkpoint file for inference.    \n",
    "    ckpt = utils.load_checkpoint(ckpt_path, model)\n",
    "\n",
    "    # change weight matrices\n",
    "    for x in model.layers_weight:\n",
    "        x.data = x * (torch.abs(x) > threshold)\n",
    "    \n",
    "    ih = ckpt['state_dict']['rnn.weight_ih_l0']\n",
    "    hh = ckpt['state_dict']['rnn.weight_hh_l0']\n",
    "    oh = ckpt['state_dict']['output_layer.weight']\n",
    "    a  = np.concatenate((ih.numpy(), hh.numpy()), axis=1)\n",
    "    h_w = np.concatenate((a, np.transpose(oh.numpy())), axis=1)\n",
    "\n",
    "    # Remove nodes with only self connections\n",
    "\n",
    "    for i, row in enumerate(h_w):\n",
    "        for j, val in enumerate(row):\n",
    "            if j < (len(row)-hps.n_bits):\n",
    "                if val!=0 and sum(abs(h_w[i]) )- abs(val)==0 and i+hps.n_bits==j:\n",
    "                    model.rnn.weight_hh_l0.data[i][i] = 0\n",
    "    h_w = []\n",
    "    for layer in model.layers_weight:\n",
    "        h_w += [torch.flatten(layer.data).cpu().numpy()]\n",
    "    h_w = np.concatenate(h_w)\n",
    "    return h_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ed\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Plot network edge density vs. $\\alpha$ values (averaged across 20 trials)](#ed)\n",
    "```plot_ED_alpha``` produces the edge density vs. alphas for both types of constrained models.  \n",
    "\n",
    "Users need to specify the values of $\\alpha$ to be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_ED_alpha(trial=hps.trial, threshold=0., print_graph=True):\n",
    "    betas = [0.0, 1.0]\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    labels = [f'L{hps.norm} Regularization', 'Distance Constrained']\n",
    "    max_y = 0\n",
    "    for beta, label in zip(betas, labels):\n",
    "        alphas = np.arange(0.0, 0.009, 0.001)\n",
    "        def fn(h_w, threshold):\n",
    "            A = (np.abs(h_w) > threshold).astype(int)\n",
    "            a_sum = A.sum()\n",
    "            # Network Connection Density is computed as nonzero_weights/total_weigths\n",
    "            valid_conn = np.round(a_sum / len(h_w), 4)\n",
    "            return valid_conn  \n",
    "        ed = []\n",
    "        ed_std = []\n",
    "        for alpha in alphas:\n",
    "            avg_conn, std_cond = average_trial(trial, fn, alpha, beta, threshold=threshold)\n",
    "            ed.append(avg_conn)\n",
    "            ed_std.append(std_cond)\n",
    "        ax.errorbar(alphas, ed, yerr=ed_std, label=f'{label}')\n",
    "        max_y = max(max_y, max(ed)+max(ed_std))\n",
    "    ax.set_xticks(alphas)\n",
    "    ax.set(xlabel=r'$\\alpha$', ylabel='Edge Density', title=r'Network Edge Density vs. $\\alpha$')\n",
    "    ax.legend(loc=0)\n",
    "    fig.savefig(f'EdgeDensityAlpha_trial{trial}.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trial\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Plot Flip-flop task trial figures](#trial)\n",
    "Function `plot_trial` generates a visual representation of the model's performance with MSE denoted.\n",
    "Users need to specify the seed of the experiment and the type of constrained models to be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trial(seed, beta):\n",
    "    files = list(glob(f'./trial{hps.trial}/seed{seed}trained_model/alpha_*_beta_{beta}/checkpoints/last.pth'))\n",
    "    for f in files:\n",
    "        alpha = f.split('alpha_')[-1].split('_beta')[0]\n",
    "        ckpt = torch.load(f, map_location='cpu')\n",
    "        inputs = ckpt[\"inputs\"]\n",
    "        outputs = ckpt[\"outputs\"]\n",
    "        targets = ckpt[\"targets\"]\n",
    "        accuracy = ckpt[\"accuracy\"][0]\n",
    "        n_bits = hps.n_bits\n",
    "        vertical_spacing = 2.5\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "        for bit_idx in range(n_bits):\n",
    "            ax.step(\n",
    "                range(inputs.shape[1]),\n",
    "                inputs[0, :, bit_idx].detach() + vertical_spacing * bit_idx,\n",
    "                color=\"#9C3D17\",\n",
    "                label=\"Inputs\",\n",
    "                linewidth=2.5,\n",
    "            )\n",
    "            ax.plot(\n",
    "                range(outputs.shape[1]),\n",
    "                outputs[0, :, bit_idx].detach() + vertical_spacing * bit_idx,\n",
    "                color=\"#00119E\",\n",
    "                label=\"Outputs\",\n",
    "                linewidth=3.5,\n",
    "            )\n",
    "            ax.plot(\n",
    "                range(targets.shape[1]),\n",
    "                targets[0, :, bit_idx].detach() + vertical_spacing * bit_idx,\n",
    "                color=\"#E8C23A\",\n",
    "                label=\"Targets\",\n",
    "                linewidth=3.5,\n",
    "            )\n",
    "\n",
    "        ax.set_yticks([(bit_idx * vertical_spacing) for bit_idx in range(n_bits)])\n",
    "        ax.set_yticklabels(\n",
    "            [\"Bit %d\" % (n_bits - bit_idx) for bit_idx in range(n_bits)],\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax.set_title(\"Trial Plot with mse = {:0.2f}\".format(accuracy), fontweight=\"bold\")\n",
    "        ax.set_xlabel(f\"Time Step (alpha_{alpha})\", fontweight=\"bold\")\n",
    "\n",
    "        fig.savefig(f'trial{hps.trial}_seed{seed}_alpha_{alpha}_beta_{beta}_trialplot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Plot connectivity vs. distance for a specific alpha](#conn)\n",
    "The ```plot_dist_conn_hres``` function outputsg a histogram of the counts of connections (excluding nodes with only self-loops) vs. the distance distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist_conn_hres(alpha, hps, threshold=0.):\n",
    "    files_seed = [np.sort(list(glob(f'./trial{hps.trial}/seed*trained_model/alpha_{alpha}_beta_0.0/checkpoints/last.pth'))).tolist(),\n",
    "                  np.sort(list(glob(f'./trial{hps.trial}/seed*trained_model/alpha_{alpha}_beta_1.0/checkpoints/last.pth'))).tolist()]\n",
    "    files = [*zip(files_seed[0], files_seed[1])]\n",
    "    bins=50\n",
    "    hist_f1 = []\n",
    "    hist_f2 = []\n",
    "    dist_f1 = []\n",
    "    dist_f2 = []\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    for f1, f2 in files:\n",
    "        for f, hist_save, dist_save in zip([f1, f2], [hist_f1, hist_f2], [dist_f1, dist_f2]):\n",
    "            _, _, dist = get_wd(model, f)\n",
    "            h_w = trim_wt(f, hps)\n",
    "            h_d = dist[np.abs(h_w) > threshold]\n",
    "            h_d = h_d[h_d != 0.]\n",
    "            dist_save.append(h_d.mean())\n",
    "            hist_save += [*h_d]\n",
    "    dist_f1 = np.median(dist_f1)\n",
    "    dist_f2 = np.median(dist_f2)\n",
    "    \n",
    "    for hist, d_mean, beta, color in zip([hist_f1,hist_f2], [dist_f1, dist_f2], [f'L{hps.norm} Regularization', 'Distance Constrained',], ['lightblue', 'orange']):\n",
    "        ax.hist(hist, bins=bins, label=f'{beta}', color=color, alpha=0.8)\n",
    "        ax.axvline(d_mean, linestyle='dashed', c=color)\n",
    "\n",
    "    ax.set(title=f'Averaged Distance Distributions vs. #Connections (a={alpha})', \n",
    "           ylabel='#Connections', xlabel='Distance Distribution')\n",
    "    ax.set_xlim([0, 3])\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    fig.savefig(f'trial{hps.trial}_alpha_{alpha}averaged_dc.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"connd\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Plot the connection distribution at each range of distances](#connd)\n",
    "Function `plot_dist_conn` plots the distribution of the medians of all experiments at a specific distance.\n",
    "Users have the option to plot an exponential curve to fit to the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, a, b, c) :\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "def curve_fit_log(xdata, ydata) :\n",
    "    \"\"\"Fit data to a power law with weights according to a log scale\"\"\"\n",
    "    # Weights according to a log scale\n",
    "    # Apply fscalex\n",
    "    logx = np.log10(xdata)\n",
    "    # Apply fscaley\n",
    "    logy = np.log10(ydata)\n",
    "    # Fit linear\n",
    "    popt_log, pcov_log = curve_fit(linlaw, logx, logy)\n",
    "    #print(popt_log, pcov_log)\n",
    "    # Apply fscaley^-1 to fitted data\n",
    "    ydatafit_log = np.power(10, linlaw(logx, *popt_log))\n",
    "    # There is no need to apply fscalex^-1 as original data is already available\n",
    "    return popt_log, pcov_log, ydatafit_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist_conn(alpha, hps, plot_curve=False, threshold=0.):\n",
    "    files_seed = [np.sort(list(glob(f'./trial{hps.trial}/seed*trained_model/alpha_{alpha}_beta_0.0/checkpoints/last.pth'))).tolist(),\n",
    "                  np.sort(list(glob(f'./trial{hps.trial}/seed*trained_model/alpha_{alpha}_beta_1.0/checkpoints/last.pth'))).tolist()]\n",
    "    files = [*zip(files_seed[0], files_seed[1])]\n",
    "    \n",
    "    width = 0.5\n",
    "    bins = np.arange(0,3.,width)\n",
    "    x_bar = (bins[1:] - bins[:-1]) / 2 + bins[:-1]\n",
    "    x_ticks = ['D1', 'D2', 'D3', 'D4', 'D5']\n",
    "    hist_f1 = []\n",
    "    hist_f2 = []\n",
    "    dist_f1 = []\n",
    "    dist_f2 = []\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    for f1, f2 in files:\n",
    "        for f, hist_save, dist_save in zip([f1, f2], [hist_f1, hist_f2], [dist_f1, dist_f2]):\n",
    "            _, _, dist = get_wd(model, f)\n",
    "            h_w = trim_wt(f, hps)\n",
    "            h_d = dist[np.abs(h_w) > threshold]\n",
    "            dist_save.append(h_d.mean())\n",
    "            \n",
    "            # This outputs the histogram of the distance of nodes which have connections, \n",
    "            # and contains the number of samples in each bin; if density is set to be \"True\", the the result is a pdf\n",
    "            counts, edges = np.histogram(h_d, bins=bins, density=False)\n",
    "            \n",
    "            hist_save.append(counts)\n",
    "    dist_f1 = np.median(dist_f1)\n",
    "    dist_f2 = np.median(dist_f2)\n",
    "    hist_f1 = np.median(np.stack(hist_f1), axis=0)\n",
    "    hist_f2 = np.median(np.stack(hist_f2), axis=0)\n",
    "    labels = ['L1 Regularization', 'Distance Constrained']\n",
    "    for hist, d_mean, label, color in zip([hist_f1,hist_f2], [dist_f1, dist_f2], labels, ['lightblue', 'orange']):\n",
    "        ax.bar(x_bar, hist, width=width, label=f'{label}', color=color, alpha=0.8)\n",
    "        ax.axvline(d_mean, linestyle='dashed', c=color)\n",
    "\n",
    "    ax.set(title=f'Levels of distance vs. #Connections (a={alpha})', \n",
    "           ylabel='#Connections', xlabel='Distance Distribution')\n",
    "    ax.set_xlim([0, 2.5])\n",
    "    ax.set_ylim([0, max(max(hist_f1), max(hist_f2))+0.1])\n",
    "    ax.set_xticks(x_bar)\n",
    "    ax.set_xticklabels(x_ticks)\n",
    "        \n",
    "    def func(x, a, b, c):\n",
    "        return a * np.exp(-b * x) + c\n",
    "    if plot_curve==True:\n",
    "        popt, pcov = curve_fit(func, x_bar, hist_f2)\n",
    "        residuals = hist_f2 - func(x_bar, *popt)\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        ss_tot = np.sum((hist_f2-np.mean(hist_f2))**2)\n",
    "        r_squared = 1 - (ss_res / ss_tot)\n",
    "        ax.plot(x_bar, func(x_bar, *popt), 'r--', label=r'$R^2$=%4.3f'% r_squared)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    fig.savefig(f'trial{hps.trial}_alpha{alpha}_averaged_leveldis_conn.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"acc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Plot the median accuracy of all experiments for each alphas](#acc)\n",
    "Function `plot_acc` plots the accuracies of each alpha, and outputs if there is a significant difference between the two types of constrained model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_acc(trial):\n",
    "    betas = [0.0, 1.0]\n",
    "    label = [f'L{hps.norm} Regularization', 'Distance Constrained']\n",
    "    colors = ['lightblue', 'orange']\n",
    "    alphas = np.arange(0.0, 0.009, 0.001)\n",
    "    pos_a = [0.5+i*1.5 for i in range(len(alphas))]\n",
    "    pos_b = [a+0.5 for a in pos_a]\n",
    "    x_t = [a+0.5/2 for a in pos_a]\n",
    "    position = [pos_a, pos_b]\n",
    "    x_ticks = [str(a) for a in alphas]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    l_acc = []\n",
    "    d_acc = []\n",
    "    def avg(files):\n",
    "        accs = []\n",
    "        for f in files:\n",
    "            ckpt = torch.load(f, map_location='cpu')\n",
    "            acc = ckpt['accuracy'][0]\n",
    "            accs.append(acc)\n",
    "        return accs\n",
    "    def add_label(violin, label):\n",
    "        color = violin[\"bodies\"][0].get_facecolor().flatten()\n",
    "        labels.append((mpatches.Patch(color=color), label))\n",
    "    labels = []\n",
    "    for beta, color, l, pos, accs in zip(betas, colors, label, position, [l_acc, d_acc]):\n",
    "        stdev = []\n",
    "        for alpha in alphas:\n",
    "            alpha = round(alpha, 5)\n",
    "            files = list(glob(f'./trial{trial}/seed*trained_model/alpha_{alpha}_beta_{beta}/checkpoints/last.pth'))\n",
    "            acc = avg(files)\n",
    "            accs.append(acc*100)\n",
    "        vp = ax.violinplot(accs, pos, points=20, widths=0.3, showmeans=False, showmedians=True, showextrema=False)\n",
    "        ax.set_xticks(x_t)\n",
    "        ax.set_xticklabels(x_ticks)\n",
    "        for i, pc in enumerate(vp['bodies']):\n",
    "            pc.set_facecolor(f'{color}')\n",
    "            pc.set_alpha(0.4)\n",
    "        add_label(vp, f'{l}')\n",
    "        quartile1, medians, quartile3 = np.percentile(accs, [25, 50, 75], axis=1)\n",
    "        ax.scatter(pos, medians, marker='D', color=color, s=25, zorder=3)\n",
    "\n",
    "    for a, b in zip(l_acc, d_acc):\n",
    "        d, p, pv = ks_test(a, b)\n",
    "        print(f'D-stat is {d}, p-value is {p}')\n",
    "    ax.set(xlabel=r\"$\\alpha$\", ylabel='mean square error')\n",
    "    plt.legend(*zip(*labels), loc=4)\n",
    "    plt.show()\n",
    "    fig.savefig(f'trial_{trial}_acc.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wd\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Weight Distribution vs. distances](#wd)\n",
    "Users need to specify the values of alphas inside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_w_d(seed):\n",
    "    alphas = [0.0, 0.001,0.002, 0.005, 0.007]\n",
    "    n_c = 2\n",
    "    n_r = len(alphas)\n",
    "    fig, axs = plt.subplots(figsize=(8, 10), nrows=n_r, ncols=n_c)\n",
    "    xlim = 3\n",
    "    fig.align_ylabels()\n",
    "    plt.setp(axs, xlim=(0,xlim))\n",
    "    for i, a in zip(range(n_r), alphas):  \n",
    "        lp = np.sort(list(glob(f'./trial{hps.trial}/seed{seed}trained_model/alpha_{a}_beta_0.0/checkpoints/last.pth')))\n",
    "        dc = np.sort(list(glob(f'./trial{hps.trial}/seed{seed}trained_model/alpha_{a}_beta_1.0/checkpoints/last.pth')))\n",
    "        il = [*zip(lp, dc)]\n",
    "        di = []\n",
    "        wi = []\n",
    "        dl = []\n",
    "        wl = []\n",
    "        model = ConstrainedModel(hps.n_bits, hps.hidden_size, hps.n_bits, hps.n_spatial_dims, hps.norm)\n",
    "        for f1, f2 in il:\n",
    "            for f, d_save, w_save in zip([f1, f2], [di, dl], [wi, wl]):\n",
    "                _, weight, dist = get_wd(model, f)\n",
    "#                 weight = trim_wt(f, hps)\n",
    "                \n",
    "                dist = dist[np.abs(weight) > 0.]\n",
    "                weight = weight[np.abs(weight) > 0.]\n",
    "                d_save.append(dist)\n",
    "                w_save.append(weight)\n",
    "            for ax, d, w, y_lim in zip([axs[i, 0], axs[i, 1]], [di, dl], [wi, wl], [max(max(np.abs(wi))), max(max(np.abs(wl)))]):\n",
    "                hb = ax.hexbin(d, w, gridsize=20, cmap=color, norm=matplotlib.colors.LogNorm(),\n",
    "                               extent=[0, xlim,-y_lim, y_lim], clim=[1, 10])\n",
    "                ax.set_ylim(-y_lim, y_lim)\n",
    "            axs[i,0].set_ylabel('weight distributions\\nalpha='+f'{a}', multialignment='center')\n",
    "            \n",
    "    \n",
    "    cax = fig.add_axes([0.92, 0.1, 0.03, 0.8])\n",
    "    cb = fig.colorbar(hb, cax=cax)\n",
    "    axs[0,0].set(title=f'l{hps.norm}-regularized')\n",
    "    axs[0,1].set(title='distance-constrained')\n",
    "    axs[n_r-1,0].set(xlabel='distance distribution')\n",
    "    axs[n_r-1,1].set(xlabel='distance distribution')\n",
    "    \n",
    "    fig.savefig(f'trial{hps.trial}seed{seed}_L{hps.norm}_distance_weight.png', dpi=600, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cdf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Plot the KS test as a CDF](#cdf)\n",
    "Users need to specify the values of alphas inside the function. The variable `xlims` needs to be adjusted accordingly for a proper visualization with which both tails can be seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_tails_w_d(seed, pnorm, xlims=None, debug=False):\n",
    "    alphas = [0.0, 0.001,0.002, 0.005, 0.007]\n",
    "    n_c = 2\n",
    "    n_r = len(alphas)\n",
    "    fig, axs = plt.subplots(figsize=(8, 10), nrows=n_r, ncols=n_c)\n",
    "    xlim = 3\n",
    "#     colors = ['lightblue', 'orange']\n",
    "#     fig.align_ylabels()\n",
    "#     plt.setp(axs, xlim=(0,xlim))\n",
    "    for i, a in zip(range(n_r), alphas):  \n",
    "        lp = np.sort(list(glob(f'./trial{hps.trial}/seed{seed}trained_model/alpha_{a}_beta_0.0/checkpoints/last.pth')))\n",
    "        dc = np.sort(list(glob(f'./trial{hps.trial}/seed{seed}trained_model/alpha_{a}_beta_1.0/checkpoints/last.pth')))\n",
    "        il = [*zip(lp, dc)]\n",
    "        di = []\n",
    "        wi = []\n",
    "        dl = []\n",
    "        wl = []\n",
    "        model = ConstrainedModel(hps.n_bits, hps.hidden_size, hps.n_bits, hps.n_spatial_dims, hps.norm)\n",
    "        for f1, f2 in il:\n",
    "            for f, d_save, w_save in zip([f1, f2], [di, dl], [wi, wl]):\n",
    "                _, weight, dist = get_wd(model, f)\n",
    "                \n",
    "                dist = dist[np.abs(weight) > 1e-5]\n",
    "                weight = weight[np.abs(weight) > 1e-5]\n",
    "                d_save.append(dist)\n",
    "                w_save.append(weight)\n",
    "                \n",
    "            for k, (ax, d, w, y_lim) in enumerate(zip([axs[i, 0], axs[i, 1]], [di, dl], [wi, wl], [max(max(np.abs(wi))), max(max(np.abs(wl)))])):\n",
    "                idxs = np.argsort(d)[0]  # We sort by distance\n",
    "                dist = d[0][idxs]\n",
    "                weights = w[0][idxs]\n",
    "                def get_tails(x):\n",
    "                    if len(x) % 2 == 1:\n",
    "                        # Odd\n",
    "                        median = x[len(x)//2] # Median corresponding to the distance (we sorted by it)\n",
    "                        median_pos = len(x)//2\n",
    "                        down_x = x[:median_pos]\n",
    "                        up_x = x[median_pos +1:]\n",
    "                    else:\n",
    "                        median = (x[len(x)//2 + 1] - x[len(x)//2]) / 2\n",
    "                        down_x = x[:len(x)//2]\n",
    "                        up_x = x[len(x)//2:]\n",
    "                    return down_x, up_x\n",
    "                \n",
    "                down_w, up_w = get_tails(weights)\n",
    "                if debug:\n",
    "                    print(down_w, up_w)\n",
    "                ax.plot(sorted(down_w), np.arange(len(down_w)) / len(down_w) , label='Lower tail')\n",
    "                ax.plot(sorted(up_w), np.arange(len(up_w)) / len(up_w), 'r', label='Upper tail')\n",
    "                \n",
    "                _,p,_ = ks_test(down_w, up_w)\n",
    "                print(p)\n",
    "                \n",
    "#                 ax.plot(down_d, up_dm)\n",
    "#                 ax.plot([0,up_dm[-1]], [0, up_dm[-1]], '--k')\n",
    "                if xlims:\n",
    "                    ax.set_xlim(xlims[0], xlims[1])\n",
    "                \n",
    "                if k == 0:\n",
    "                    axs[i,k].set_ylabel('Probability\\nalpha='+f'{a}\\np-val: {p:.6f}', multialignment='center')\n",
    "                else:\n",
    "                    axs[i,k].set_ylabel(f'p-val: {p:.6f}', multialignment='center')\n",
    "    fig.savefig(f'trial{hps.trial}seed{seed}_L{hps.norm}_wd_stat.png', dpi=600, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
